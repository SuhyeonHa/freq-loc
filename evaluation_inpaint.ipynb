{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee43cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, AutoencoderKL\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize_img,\n",
    "])\n",
    "\n",
    "def load_img(path, transforms=None):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = transforms(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def norm_tensor(tensor):\n",
    "    t = tensor.clone().detach()\n",
    "    \n",
    "    min_val = t.min()\n",
    "    max_val = t.max()\n",
    "\n",
    "    tensor_norm = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    print(f\"Tensor normalized: min={tensor_norm.min()}, max={tensor_norm.max()}\")\n",
    "    \n",
    "    return tensor_norm, min_val, max_val\n",
    "\n",
    "def denorm_tensor(tensor, original_min=None, original_max=None):\n",
    "    t = tensor.clone().detach()\n",
    "\n",
    "    return t * (original_max - original_min) + original_min\n",
    "\n",
    "def create_random_mask(img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "    _, _, height, width = img_pt.shape\n",
    "    mask_area = int(height * width * mask_percentage)\n",
    "    masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "    if mask_percentage >= 0.999:\n",
    "        # Full mask for entire image\n",
    "        return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "    for ii in range(num_masks):\n",
    "        placed = False\n",
    "        attempts = 0\n",
    "        while not placed and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            max_dim = int(mask_area ** 0.5)\n",
    "            mask_width = random.randint(1, max_dim)\n",
    "            mask_height = mask_area // mask_width\n",
    "\n",
    "            # Allow broader aspect ratios for larger masks\n",
    "            aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "            if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                if mask_height <= height and mask_width <= width:\n",
    "                    x_start = random.randint(0, width - mask_width)\n",
    "                    y_start = random.randint(0, height - mask_height)\n",
    "                    overlap = False\n",
    "                    for jj in range(ii):\n",
    "                        if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    if not overlap:\n",
    "                        masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                        placed = True\n",
    "\n",
    "        if not placed:\n",
    "            # Fallback: just fill a central region if all attempts fail\n",
    "            print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "            center_h = height // 2\n",
    "            center_w = width // 2\n",
    "            half_area = int((mask_area // 2) ** 0.5)\n",
    "            h_half = min(center_h, half_area)\n",
    "            w_half = min(center_w, half_area)\n",
    "            masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "    return masks.to(img_pt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ad4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Hyperparameters and configuration settings for FreqMark.\"\"\"\n",
    "    def __init__(self):\n",
    "        # --- System & Paths ---\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.image_path = '/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png'\n",
    "\n",
    "        # --- Model Configurations ---\n",
    "        self.vae_model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "        self.vae_subfolder = \"vae\"\n",
    "        self.dino_model_repo = 'facebookresearch/dinov2'\n",
    "        self.dino_model_name = 'dinov2_vits14'\n",
    "        \n",
    "        # --- Image Size Parameters ---\n",
    "        self.vae_image_size = 512\n",
    "        self.dino_image_size = 224\n",
    "        self.transform = transforms.Compose([\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # --- FreqMark Core Parameters ---\n",
    "        self.message_bits = 48\n",
    "        self.feature_dim = 384\n",
    "        self.margin = 1.0\n",
    "        self.grid_size = 16\n",
    "        self.num_patches = self.grid_size*self.grid_size\n",
    "\n",
    "        # --- Optimization Parameters ---\n",
    "        self.lr = 2.0\n",
    "        self.steps = 400\n",
    "        self.lambda_p = 0.05\n",
    "        self.lambda_i = 0.25\n",
    "\n",
    "        # --- Robustness Parameters ---\n",
    "        self.eps1_std = 0.25 \n",
    "        self.eps2_std = 0.06\n",
    "        \n",
    "        # --- Demo/Evaluation Parameters ---\n",
    "        self.batch_size = 4\n",
    "        self.num_test_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqMark:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # Initialize networks\n",
    "        self.vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"vae\").to(self.args.device)\n",
    "        self.image_encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(self.args.device)\n",
    "\n",
    "        # Freeze all networks\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Pre-define direction vectors\n",
    "        self.direction_vectors = self._init_direction_vectors()\n",
    "    \n",
    "        self.mu = self.args.margin      # Hinge loss margin\n",
    "        \n",
    "        # Noise parameters for robustness\n",
    "        self.args.eps1_std = 0.25  # Latent noise\n",
    "        self.args.eps2_std = 0.06  # Pixel noise\n",
    "    \n",
    "    def _init_direction_vectors(self) -> torch.Tensor:\n",
    "        \"\"\"Initialize direction vectors as described in paper\"\"\"\n",
    "        # binary bit for each patch\n",
    "        vectors = torch.zeros(1, self.args.feature_dim)\n",
    "        for i in range(1):\n",
    "            vectors[i, self.args.feature_dim-1] = 1.0  # One-hot encoding\n",
    "        return vectors.to(self.args.device)\n",
    "    \n",
    "    def _create_random_mask(self, img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "        _, _, height, width = img_pt.shape\n",
    "        mask_area = int(height * width * mask_percentage)\n",
    "        masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "        if mask_percentage >= 0.999:\n",
    "            # Full mask for entire image\n",
    "            return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "        for ii in range(num_masks):\n",
    "            placed = False\n",
    "            attempts = 0\n",
    "            while not placed and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "\n",
    "                max_dim = int(mask_area ** 0.5)\n",
    "                mask_width = random.randint(1, max_dim)\n",
    "                mask_height = mask_area // mask_width\n",
    "\n",
    "                # Allow broader aspect ratios for larger masks\n",
    "                aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "                if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                    if mask_height <= height and mask_width <= width:\n",
    "                        x_start = random.randint(0, width - mask_width)\n",
    "                        y_start = random.randint(0, height - mask_height)\n",
    "                        overlap = False\n",
    "                        for jj in range(ii):\n",
    "                            if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                                overlap = True\n",
    "                                break\n",
    "                        if not overlap:\n",
    "                            masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                            placed = True\n",
    "\n",
    "            if not placed:\n",
    "                # Fallback: just fill a central region if all attempts fail\n",
    "                print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "                center_h = height // 2\n",
    "                center_w = width // 2\n",
    "                half_area = int((mask_area // 2) ** 0.5)\n",
    "                h_half = min(center_h, half_area)\n",
    "                w_half = min(center_w, half_area)\n",
    "                masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "        return masks.to(img_pt.device)\n",
    "\n",
    "    def vae_recon(self, image: torch.Tensor, iter: int):\n",
    "        \"\"\"VAE reconstruction. Inputs are outputs are 512x512\"\"\"\n",
    "        latent = self.vae.encode(2*image-1).latent_dist.sample()\n",
    "        reconstructed = self.vae.decode(latent).sample\n",
    "        reconstructed = (reconstructed + 1) / 2\n",
    "        for _ in range(iter-1):\n",
    "            latent = self.vae.encode(2*reconstructed-1).latent_dist.sample()\n",
    "            reconstructed = self.vae.decode(latent).sample\n",
    "            reconstructed = (reconstructed + 1) / 2\n",
    "        return reconstructed\n",
    "\n",
    "    def embed_watermark(self, original: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed watermark in image using latent frequency space optimization\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor [B, C, H, W]\n",
    "            message: Binary message {-1, 1} [B, message_bits]\n",
    "        \n",
    "        Returns:\n",
    "            Watermarked image tensor\n",
    "        \"\"\"\n",
    "        original = original.to(self.args.device)\n",
    "        # message = message.to(self.device)\n",
    "        \n",
    "        # Step 1: Encode image to latent space\n",
    "        image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "        latent = self.vae.encode(2*image-1).latent_dist.sample() # [-1, 1], [B,4,64,64]\n",
    "        \n",
    "        # Step 2: Transform to frequency domain\n",
    "        latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "        \n",
    "        # Step 3: Initialize perturbation (trainable parameter)\n",
    "        delta_m = torch.zeros_like(latent_fft, requires_grad=True)\n",
    "        optimizer = optim.Adam([delta_m], lr=self.args.lr)\n",
    "        \n",
    "        # Training loop\n",
    "        for step in range(self.args.steps):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = self._create_random_mask(image, num_masks=1, mask_percentage=self.args.mask_percentage)\n",
    "            mask = mask.to(self.args.device)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            # Add perturbation in frequency domain\n",
    "            perturbed_fft = latent_fft + delta_m\n",
    "            \n",
    "            # Transform back to spatial domain\n",
    "            perturbed_latent = torch.fft.ifft2(perturbed_fft, dim=(-2, -1)).real\n",
    "            \n",
    "            # Generate watermarked image\n",
    "            watermarked_image = self.vae.decode(perturbed_latent).sample\n",
    "            watermarked_image = (watermarked_image + 1) / 2\n",
    "\n",
    "            masked = watermarked_image * mask + (1 - mask) * image\n",
    "\n",
    "            # # Add robustness noise during training\n",
    "            # eps1 = torch.randn_like(perturbed_latent) * self.args.eps1_std\n",
    "            # eps2 = torch.randn_like(watermarked_image) * self.args.eps2_std\n",
    "            \n",
    "            # # Perturbed versions for robustness\n",
    "            # perturbed_latent_1 = perturbed_latent + eps1\n",
    "            # watermarked_image_1 = self.vae.decode(perturbed_latent_1).sample\n",
    "            # watermarked_image_1 = (watermarked_image_1 + 1) / 2\n",
    "            # masked_1 = watermarked_image_1 * mask + (1 - mask) * image\n",
    "\n",
    "            # watermarked_image_2 = watermarked_image + eps2\n",
    "            # masked_2 = watermarked_image_2 * mask + (1 - mask) * image\n",
    "\n",
    "            # Perturbed versions for fragility\n",
    "            # watermarked_image_3 = self.vae_recon(watermarked_image, iter=3)\n",
    "            # masked_3 = watermarked_image_3 * mask + (1 - mask) * image\n",
    "\n",
    "            # Compute losses\n",
    "            image = F.interpolate(original, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            masked = F.interpolate(masked, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            # masked_1 = F.interpolate(masked_1, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            # masked_2 = F.interpolate(masked_2, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            # masked_3 = F.interpolate(masked_3, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            watermarked_image = F.interpolate(watermarked_image, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            # loss_m1 = self._message_loss(watermarked_image, message)\n",
    "            # loss_m2 = self._message_loss(watermarked_image_1, message)\n",
    "            # loss_m3 = self._message_loss(watermarked_image_2, message)\n",
    "            \n",
    "            loss_m = self._mask_loss(masked, mask)\n",
    "            # loss_m1 = self._mask_loss(masked_1, mask)\n",
    "            # loss_m2 = self._mask_loss(masked_2, mask)\n",
    "            # loss_m3 = self._mask_loss(masked_3, mask)\n",
    "\n",
    "            loss_psnr = self._psnr_loss(watermarked_image, image)\n",
    "            loss_lpips = self._lpips_loss(watermarked_image, image)\n",
    "\n",
    "            # loss_reg = torch.mean(delta_m.real**2)\n",
    "            \n",
    "            # Combined loss (Equation 10 from paper)\n",
    "            total_loss = (loss_m + # loss_m1 + loss_m2 + #loss_m3 + \n",
    "                         self.args.lambda_p * loss_psnr + \n",
    "                         self.args.lambda_i * loss_lpips)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                psnr_val = self._compute_psnr(watermarked_image, image)\n",
    "                print(f\"Step {step}, Loss: {total_loss.item():.4f}, PSNR: {psnr_val:.2f}\")\n",
    "        \n",
    "        # Final watermarked image\n",
    "        final_fft = latent_fft + delta_m\n",
    "        final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "        final_watermarked = self.vae.decode(final_latent).sample\n",
    "        final_watermarked = (final_watermarked + 1) / 2\n",
    "        \n",
    "        return final_watermarked.detach()\n",
    "    \n",
    "    def decode_watermark(self, watermarked_image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode watermark from image using pre-trained image encoder\n",
    "        \n",
    "        Args:\n",
    "            watermarked_image: Watermarked image tensor [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Decoded message {-1, 1} [B, message_bits]\n",
    "        \"\"\"\n",
    "        watermarked_image = watermarked_image.to(self.args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Extract features using image encoder\n",
    "            # features = self.image_encoder(watermarked_image) # [1, 256, 384]\n",
    "            features = self.image_encoder.get_intermediate_layers(watermarked_image)[0] # [1, 256, 384]\n",
    "            \n",
    "            # Compute dot products with direction vectors\n",
    "            dot_products = torch.matmul(features, self.direction_vectors.T) # [1, 256, 384]*[1, 384, 256] -> [1, 256, 1]\n",
    "            \n",
    "            B = dot_products.shape[0]\n",
    "            H = W = int(dot_products.shape[1] ** 0.5)\n",
    "            grid = dot_products.view(B, H, W).unsqueeze(0) # [1, 256, 1] -> [1, 1, 16, 16]\n",
    "            grid = F.interpolate(grid, size=self.args.dino_image_size, mode='bilinear', align_corners=False)\n",
    "        return grid\n",
    "    \n",
    "    def _message_loss(self, watermarked_image: torch.Tensor, message: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Hinge loss for message embedding (Equation 7)\"\"\"\n",
    "        features = self.image_encoder(watermarked_image)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        \n",
    "        # Hinge loss with margin\n",
    "        projections = dot_products * message\n",
    "        loss = torch.clamp(self.mu - projections, min=0).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _mask_loss(self, watermarked_image: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss based on patch-wise watermark detection to enforce a global watermark presence.\n",
    "        The ground truth mask is implicitly all-ones, meaning the loss is minimized when all patches\n",
    "        correctly embed the watermark.\n",
    "        \"\"\"\n",
    "        image_for_dino = F.interpolate(watermarked_image, \n",
    "                                       size=(self.args.dino_image_size, self.args.dino_image_size), \n",
    "                                       mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        features = self.image_encoder.get_intermediate_layers(image_for_dino, n=1)[0] # [B, Num_Patches, Feature_Dim]\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        B = dot_products.shape[0]\n",
    "        H = W = int(dot_products.shape[1] ** 0.5)\n",
    "        grid = dot_products.view(B, H, W).unsqueeze(0)\n",
    "        # grid = dot_products.view(self.args.grid_size, self.args.grid_size).unsqueeze(0).unsqueeze(0) # [1, 256, 1] -> [1, 1, 14, 14]\n",
    "        grid = F.interpolate(grid, size=self.args.dino_image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        loss = F.binary_cross_entropy_with_logits(grid, gt_mask)\n",
    "        return loss\n",
    "    \n",
    "    def _psnr_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative PSNR loss (Equation 5)\"\"\"\n",
    "        mse = F.mse_loss(pred, target)\n",
    "        psnr = -10 * torch.log10(mse + 1e-8)\n",
    "        return -psnr  # Negative for minimization\n",
    "    \n",
    "    def _lpips_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simplified LPIPS-like loss\"\"\"\n",
    "        # Simplified perceptual loss using L2 in feature space\n",
    "        pred_gray = 0.299 * pred[:, 0] + 0.587 * pred[:, 1] + 0.114 * pred[:, 2]\n",
    "        target_gray = 0.299 * target[:, 0] + 0.587 * target[:, 1] + 0.114 * target[:, 2]\n",
    "        return F.mse_loss(pred_gray, target_gray)\n",
    "    \n",
    "    def _compute_psnr(self, pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"Compute PSNR between images\"\"\"\n",
    "        mse = F.mse_loss(pred, target).item()\n",
    "        if mse == 0:\n",
    "            return 100.0\n",
    "        return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "    \n",
    "    def compute_bit_accuracy(self, original_message: torch.Tensor, \n",
    "                           decoded_message: torch.Tensor) -> float:\n",
    "        \"\"\"Compute bit accuracy between original and decoded messages\"\"\"\n",
    "        matches = (original_message == decoded_message).float()\n",
    "        return matches.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/random_vec/0002.png\"\n",
    "seed = 42\n",
    "proportion_masked = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7288e47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e58925fbb7947d5ad56bab0d35158cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7945e-02,  6.7256e-02,  3.4227e-02, -5.3707e-02,  2.4351e-03,\n",
      "         -6.2954e-02,  4.3494e-02,  6.4512e-03, -4.3260e-02, -2.3343e-02,\n",
      "         -7.0346e-02,  8.2533e-02,  4.5617e-02, -6.7230e-04,  4.4777e-02,\n",
      "          7.0504e-02,  5.4996e-02,  5.3874e-02, -7.2841e-02, -9.6491e-02,\n",
      "         -3.6807e-02, -5.8929e-02, -5.0547e-02, -3.5983e-02,  5.8294e-02,\n",
      "         -5.8480e-02, -4.1023e-02, -6.7289e-03,  3.6143e-02,  3.7645e-02,\n",
      "          2.0533e-02, -2.5877e-02,  7.9669e-02,  8.6336e-02, -4.6079e-02,\n",
      "          2.9880e-02, -2.7144e-02,  1.0767e-01,  4.6213e-02,  7.8054e-02,\n",
      "          1.9284e-02,  6.7961e-02,  2.1781e-03, -8.9152e-03,  8.5987e-02,\n",
      "          8.2856e-02,  7.6484e-02, -1.7998e-02,  5.9301e-04, -5.8218e-03,\n",
      "          4.0364e-02, -1.3834e-02,  2.2900e-03, -3.7656e-02,  1.7204e-01,\n",
      "          2.6658e-02, -2.5132e-02,  3.5131e-02, -2.8146e-03,  5.1494e-02,\n",
      "          1.4336e-02, -4.6240e-02,  4.6898e-02, -1.4466e-02,  2.9323e-02,\n",
      "         -5.6262e-02, -4.7408e-02,  1.2086e-02, -1.1421e-01, -8.5554e-03,\n",
      "          8.1869e-02,  2.1994e-02, -4.2336e-02, -8.9776e-03,  1.2347e-02,\n",
      "         -8.8264e-03,  5.0410e-03, -4.9536e-02, -7.2977e-02,  6.4234e-02,\n",
      "          4.5877e-02,  6.7666e-03,  5.3200e-02,  9.4992e-02, -8.9368e-03,\n",
      "          9.7880e-03, -7.3316e-02,  2.1488e-02,  2.9167e-02,  1.1198e-02,\n",
      "         -4.4258e-03,  5.6509e-02, -4.9259e-02,  5.0003e-02,  2.5727e-02,\n",
      "         -4.6947e-03,  6.5517e-02,  1.0468e-01,  3.5481e-02, -7.0734e-02,\n",
      "         -2.0221e-02,  7.2501e-02, -5.6452e-03, -8.0041e-02, -7.7108e-03,\n",
      "         -5.7616e-02,  9.3519e-03, -1.2606e-01, -8.9349e-03,  4.2834e-02,\n",
      "         -7.1310e-02,  7.4735e-03,  1.1134e-02, -1.0973e-01,  1.5331e-02,\n",
      "         -3.8581e-02, -5.3968e-02, -9.3885e-03, -1.0624e-02, -2.6428e-02,\n",
      "         -2.5212e-02, -3.1308e-02, -4.8762e-02, -2.1484e-02, -3.4651e-02,\n",
      "         -1.7176e-02, -7.9098e-02, -5.0411e-02,  1.2447e-02,  4.4607e-03,\n",
      "         -4.2249e-02,  2.5617e-02, -3.6385e-03, -3.9643e-02, -3.8643e-02,\n",
      "         -1.1249e-01,  7.7470e-02, -2.8462e-03, -4.3295e-02,  6.1347e-02,\n",
      "         -2.9446e-02,  3.1080e-02,  4.7390e-03, -3.0385e-02, -6.8236e-02,\n",
      "          5.8651e-02, -8.4814e-02, -1.0019e-03, -4.3548e-02, -4.6419e-02,\n",
      "          5.5151e-03, -4.2168e-02, -1.7591e-02, -9.6461e-02, -1.9567e-02,\n",
      "          3.2931e-03, -2.4510e-02, -6.4298e-02,  1.2846e-01,  4.7924e-02,\n",
      "          2.2739e-02,  3.0701e-02, -4.3705e-02,  4.8260e-02,  4.1408e-02,\n",
      "         -1.6722e-02,  4.7887e-02, -3.8459e-02, -1.6873e-02, -3.4115e-02,\n",
      "         -8.5321e-02, -2.2827e-02,  9.2397e-02, -1.1757e-01,  8.2800e-02,\n",
      "         -1.0486e-01,  2.9522e-02,  3.4271e-02, -2.8904e-02,  6.2694e-04,\n",
      "          2.5219e-02, -8.3978e-03,  1.9372e-02, -1.0623e-04, -5.4338e-02,\n",
      "         -1.9337e-02,  6.3783e-02, -1.0113e-01,  9.8149e-03,  5.0316e-02,\n",
      "         -6.2076e-03,  8.9066e-02,  4.9736e-02,  3.2174e-02, -4.8808e-02,\n",
      "          1.5056e-02, -2.0643e-02,  8.3612e-02,  2.2519e-02, -8.9736e-03,\n",
      "          1.8351e-02,  3.0417e-02, -2.1129e-02, -2.1499e-02,  1.4482e-01,\n",
      "          6.3070e-02, -1.7185e-02,  1.4010e-04, -5.6998e-03,  4.1263e-02,\n",
      "         -5.6033e-02,  5.9436e-02, -2.7478e-02,  7.2093e-03, -5.7858e-02,\n",
      "         -2.8084e-02,  8.6338e-03, -4.2361e-02,  3.1175e-02, -2.0577e-02,\n",
      "         -8.3863e-02,  4.2332e-02, -1.6855e-02,  1.1160e-02,  6.0268e-02,\n",
      "         -1.6810e-02,  6.8096e-02,  2.4171e-02,  4.6602e-03,  1.7652e-02,\n",
      "         -3.3963e-02, -1.5444e-02,  2.2451e-02,  2.7925e-02, -1.2093e-01,\n",
      "         -1.4625e-02, -2.5135e-02, -3.1008e-04,  1.2579e-02, -7.2920e-02,\n",
      "         -5.1395e-02,  5.5555e-02,  1.1334e-02, -3.7808e-02,  1.1354e-02,\n",
      "          3.5587e-02, -7.1285e-02,  4.8510e-02, -4.5798e-02,  3.4787e-02,\n",
      "          8.3659e-03,  4.9590e-02, -7.9823e-03,  6.1147e-02,  4.3732e-02,\n",
      "         -1.8910e-02,  2.9735e-02, -9.1827e-02,  2.5505e-03,  3.4723e-02,\n",
      "         -3.4595e-02,  8.3779e-03, -6.3963e-02,  2.1052e-02, -2.1236e-02,\n",
      "         -1.3454e-02,  3.6532e-02,  6.4150e-02,  3.6647e-02, -2.6366e-03,\n",
      "          8.7955e-02, -7.9528e-03, -6.0143e-02,  7.7312e-02, -2.9092e-03,\n",
      "         -2.8371e-02,  7.0603e-02, -5.7079e-02, -7.6621e-02,  3.5198e-02,\n",
      "          1.1648e-02, -1.9117e-02, -5.9539e-02,  1.1513e-02, -6.2778e-02,\n",
      "          1.5541e-02,  1.0096e-02, -3.6816e-02, -1.0164e-02,  5.1107e-02,\n",
      "          3.9785e-02, -3.3256e-02,  1.2603e-01, -2.6221e-02, -4.1545e-02,\n",
      "         -8.1803e-03, -2.6491e-02, -2.2151e-02,  7.6232e-02,  4.6861e-02,\n",
      "          6.6964e-03, -9.7445e-02, -2.1178e-02, -4.7978e-03, -9.8594e-03,\n",
      "         -2.7312e-03, -3.4202e-02,  5.0473e-02, -5.4340e-02,  2.3041e-02,\n",
      "         -5.0499e-02, -1.5994e-02,  9.1480e-02,  5.8893e-03,  5.2640e-03,\n",
      "          1.3289e-01, -1.2354e-01,  8.9476e-03,  3.4564e-02,  1.0452e-01,\n",
      "         -2.0185e-02,  9.9943e-03,  2.6693e-02,  6.1075e-03,  4.8331e-02,\n",
      "         -5.1798e-02,  1.1030e-01, -5.9484e-02, -3.5677e-02, -8.7368e-02,\n",
      "         -3.9927e-02,  8.3146e-02, -4.9512e-03, -2.3777e-02, -4.7500e-02,\n",
      "         -1.7715e-02, -7.8063e-02,  8.5672e-02,  1.8823e-03,  1.7834e-02,\n",
      "          4.3477e-02,  6.7909e-02, -6.0954e-03, -2.7348e-02,  2.4741e-02,\n",
      "          4.0364e-02,  1.0330e-01, -3.2372e-04,  6.7418e-02, -7.4927e-02,\n",
      "          4.0089e-02, -4.9921e-02, -2.2844e-02, -3.3166e-02,  2.7821e-02,\n",
      "          4.4441e-02,  9.0416e-02, -4.4142e-02, -1.8354e-02,  2.7497e-02,\n",
      "          6.4934e-02,  3.2293e-02,  2.8492e-02,  5.3689e-02, -7.5095e-03,\n",
      "         -3.9468e-02,  1.4671e-02,  1.4424e-02,  6.2267e-02, -6.7646e-02,\n",
      "         -6.2738e-03,  4.0344e-03,  2.2152e-02,  1.9965e-02, -4.9869e-02,\n",
      "          9.2134e-03, -4.1529e-02,  1.0369e-01, -9.5088e-02,  1.9519e-02,\n",
      "          1.8303e-02,  1.2626e-01,  2.9414e-03, -1.0202e-02]], device='cuda:0')\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be39344504ff478d900c4c85da38e0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"sd-legacy/stable-diffusion-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir='/mnt/nas5/suhyeon/caches'\n",
    ").to(device)\n",
    "\n",
    "args = Params()\n",
    "freqmark = FreqMark(args=args)\n",
    "\n",
    "secret_key = np.load('random_vecs.npy')\n",
    "freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "print(freqmark.direction_vectors)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "watermarked = load_img(img_path, transforms=args.transform)\n",
    "original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "img_norm, min_norm, max_norm = norm_tensor(watermarked)\n",
    "img_edit_pil = pipe(prompt=\"\", image=img_norm, mask_image=mask, generator=generator).images[0]\n",
    "img_edit = to_tensor(img_edit_pil)\n",
    "img_edit = img_edit.unsqueeze(0).to(device)\n",
    "\n",
    "img_edit = denorm_tensor(img_edit, min_norm, max_norm)  # [1, 3, H, W]\n",
    "# img_edit_m = img_edit * mask + watermarked * (1 - mask)\n",
    "\n",
    "img_edit_m = F.interpolate(img_edit, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "decoded_batch = freqmark.decode_watermark(img_edit_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bad2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "mask = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "save_image(original, \"eval_original.png\")\n",
    "save_image(watermarked, \"eval_watermarked.png\")\n",
    "save_image(img_edit_m, \"eval_edited_w_mask.png\")\n",
    "save_image(decoded_batch, \"eval_localized.png\")\n",
    "save_image(1-mask, \"eval_mask.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
