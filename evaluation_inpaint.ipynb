{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee43cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, AutoencoderKL\n",
    "import timm\n",
    "\n",
    "class Denormalize(transforms.Normalize):\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.tensor(mean)\n",
    "        std = torch.tensor(std)\n",
    "        self.mean_rev = -mean / std\n",
    "        self.std_rev = 1 / std\n",
    "        super().__init__(mean=self.mean_rev, std=self.std_rev)\n",
    "\n",
    "norm_dino = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "denorm_dino = Denormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize_img,\n",
    "])\n",
    "\n",
    "def load_img(path, transforms=None):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = transforms(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def norm_tensor(tensor):\n",
    "    t = tensor.clone().detach()\n",
    "    \n",
    "    min_val = t.min()\n",
    "    max_val = t.max()\n",
    "\n",
    "    tensor_norm = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    print(f\"Tensor normalized: min={tensor_norm.min()}, max={tensor_norm.max()}\")\n",
    "    \n",
    "    return tensor_norm, min_val, max_val\n",
    "\n",
    "def denorm_tensor(tensor, original_min=None, original_max=None):\n",
    "    t = tensor.clone().detach()\n",
    "\n",
    "    return t * (original_max - original_min) + original_min\n",
    "\n",
    "def create_random_mask(img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "    _, _, height, width = img_pt.shape\n",
    "    mask_area = int(height * width * mask_percentage)\n",
    "    masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "    if mask_percentage >= 0.999:\n",
    "        # Full mask for entire image\n",
    "        return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "    for ii in range(num_masks):\n",
    "        placed = False\n",
    "        attempts = 0\n",
    "        while not placed and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            max_dim = int(mask_area ** 0.5)\n",
    "            mask_width = random.randint(1, max_dim)\n",
    "            mask_height = mask_area // mask_width\n",
    "\n",
    "            # Allow broader aspect ratios for larger masks\n",
    "            aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "            if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                if mask_height <= height and mask_width <= width:\n",
    "                    x_start = random.randint(0, width - mask_width)\n",
    "                    y_start = random.randint(0, height - mask_height)\n",
    "                    overlap = False\n",
    "                    for jj in range(ii):\n",
    "                        if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    if not overlap:\n",
    "                        masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                        placed = True\n",
    "\n",
    "        if not placed:\n",
    "            # Fallback: just fill a central region if all attempts fail\n",
    "            print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "            center_h = height // 2\n",
    "            center_w = width // 2\n",
    "            half_area = int((mask_area // 2) ** 0.5)\n",
    "            h_half = min(center_h, half_area)\n",
    "            w_half = min(center_w, half_area)\n",
    "            masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "    return masks.to(img_pt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ad4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Hyperparameters and configuration settings for FreqMark.\"\"\"\n",
    "    def __init__(self):\n",
    "        # --- System & Paths ---\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.image_path = '/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png'\n",
    "\n",
    "        # --- Model Configurations ---\n",
    "        self.vae_model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "        self.vae_subfolder = \"vae\"\n",
    "        self.dino_model_repo = 'facebookresearch/dinov2'\n",
    "        self.dino_model_name = 'dinov2_vits14'\n",
    "        \n",
    "        # --- Image Size Parameters ---\n",
    "        self.vae_image_size = 512\n",
    "        self.dino_image_size = 224\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.vae_image_size),\n",
    "            transforms.CenterCrop(self.vae_image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # --- FreqMark Core Parameters ---\n",
    "        self.message_bits = 2\n",
    "        self.feature_dim = 192\n",
    "        self.margin = 1.0\n",
    "        self.grid_size = 28\n",
    "        # self.num_patches = self.grid_size*self.grid_size\n",
    "\n",
    "        # --- Optimization Parameters ---\n",
    "        self.lr = 2.0\n",
    "        self.steps = 400\n",
    "        self.lambda_p = 0.05\n",
    "        self.lambda_i = 0.25\n",
    "\n",
    "        # --- Robustness Parameters ---\n",
    "        self.eps1_std = 0.25 \n",
    "        self.eps2_std = 0.06\n",
    "        \n",
    "        # --- Demo/Evaluation Parameters ---\n",
    "        self.batch_size = 4\n",
    "        self.num_test_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqMark:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # Initialize networks\n",
    "        # self.vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"vae\").to(self.args.device)\n",
    "        # self.image_encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(self.args.device)\n",
    "        self.image_encoder = timm.create_model(\n",
    "            'convnext_tiny',\n",
    "            pretrained=True,\n",
    "            features_only=True,\n",
    "        ).to(self.args.device)\n",
    "        self.pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "            \"sd-legacy/stable-diffusion-inpainting\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            cache_dir='/mnt/nas5/suhyeon/caches'\n",
    "        ).to(self.args.device)\n",
    "\n",
    "        # Freeze all networks\n",
    "        # for param in self.vae.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pipe.vae.requires_grad_(False)\n",
    "        self.pipe.unet.requires_grad_(False)\n",
    "        self.pipe.text_encoder.requires_grad_(False)\n",
    "        self.pipe.vae.eval()\n",
    "        self.pipe.unet.eval()\n",
    "        self.pipe.text_encoder.eval()\n",
    "\n",
    "        self.pipe.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        # Pre-define direction vectors\n",
    "        # self.direction_vectors = torch.load('./sensitive_vec.pt')\n",
    "        # self.direction_vectors = torch.randn(self.args.feature_dim)\n",
    "        # self.direction_vectors = F.normalize(self.direction_vectors, p=2, dim=0)\n",
    "        # self.direction_vectors = self.direction_vectors.unsqueeze(0).to(self.args.device)\n",
    "        # torch.save(self.direction_vectors, './random_vec.pt')\n",
    "        self.direction_vectors = torch.load('./random_vec.pt').to(self.args.device)\n",
    "    \n",
    "        self.mu = self.args.margin      # Hinge loss margin\n",
    "        self.num_patches = (self.args.dino_image_size // 14) ** 2\n",
    "        # self.secret_key = torch.randint(0, 2, (1, self.num_patches, 1), device=self.args.device) * 2 - 1 # {-1, 1}\n",
    "        # self.secret_key = torch.randint(0, 2, (1, args.grid_size*args.grid_size, 1), device=self.args.device) * 2 - 1 # {-1, 1}\n",
    "        # self.secret_key = torch.zeros((1, args.grid_size*args.grid_size, 1), dtype=torch.int, device=self.args.device)# {-1, 1}\n",
    "        # torch.save(self.secret_key, './secret_key.pt')\n",
    "        # self.secret_key = torch.load('./secret_key.pt').to(self.args.device)\n",
    "\n",
    "\n",
    "    # def _init_direction_vectors(self) -> torch.Tensor:\n",
    "    #     \"\"\"Initialize direction vectors as described in paper\"\"\"\n",
    "    #     # binary bit for each patch\n",
    "    #     # vectors = torch.zeros(1, self.args.feature_dim)\n",
    "    #     # for i in range(1):\n",
    "    #     #     vectors[i, self.args.feature_dim-1] = 1.0  # One-hot encoding\n",
    "    #     # print(vectors)\n",
    "    #     # return vectors.to(self.args.device)\n",
    "    #     # random_vector = torch.randn(self.args.feature_dim, device=self.args.device)\n",
    "    #     # normalized_vector = F.normalize(random_vector, p=2, dim=0)\n",
    "    #     # return normalized_vector.unsqueeze(0)\n",
    "    #     self.direction_vectors = torch.load('./insensitive_vec.pt')\n",
    "\n",
    "    def norm_tensor(self, tensor):\n",
    "        t = tensor.clone().detach()\n",
    "        \n",
    "        min_val = t.min()\n",
    "        max_val = t.max()\n",
    "\n",
    "        tensor_norm = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "        # print(f\"Tensor normalized: min={tensor_norm.min()}, max={tensor_norm.max()}\")\n",
    "        \n",
    "        return tensor_norm, min_val, max_val\n",
    "\n",
    "    def denorm_tensor(self, tensor, original_min=None, original_max=None):\n",
    "        t = tensor.clone().detach()\n",
    "\n",
    "        return t * (original_max - original_min) + original_min\n",
    "\n",
    "    def _create_random_mask(self, img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "        _, _, height, width = img_pt.shape\n",
    "        mask_area = int(height * width * mask_percentage)\n",
    "        masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "        if mask_percentage >= 0.999:\n",
    "            # Full mask for entire image\n",
    "            return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "        for ii in range(num_masks):\n",
    "            placed = False\n",
    "            attempts = 0\n",
    "            while not placed and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "\n",
    "                max_dim = int(mask_area ** 0.5)\n",
    "                mask_width = random.randint(1, max_dim)\n",
    "                mask_height = mask_area // mask_width\n",
    "\n",
    "                # Allow broader aspect ratios for larger masks\n",
    "                aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "                if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                    if mask_height <= height and mask_width <= width:\n",
    "                        x_start = random.randint(0, width - mask_width)\n",
    "                        y_start = random.randint(0, height - mask_height)\n",
    "                        overlap = False\n",
    "                        for jj in range(ii):\n",
    "                            if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                                overlap = True\n",
    "                                break\n",
    "                        if not overlap:\n",
    "                            masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                            placed = True\n",
    "\n",
    "            if not placed:\n",
    "                # Fallback: just fill a central region if all attempts fail\n",
    "                print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "                center_h = height // 2\n",
    "                center_w = width // 2\n",
    "                half_area = int((mask_area // 2) ** 0.5)\n",
    "                h_half = min(center_h, half_area)\n",
    "                w_half = min(center_w, half_area)\n",
    "                masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "        return masks.to(img_pt.device)\n",
    "\n",
    "    def vae_recon(self, image: torch.Tensor, iter: int):\n",
    "        \"\"\"VAE reconstruction. Inputs are outputs are 512x512\"\"\"\n",
    "        latent = self.vae.encode(2*image-1).latent_dist.sample()\n",
    "        reconstructed = self.vae.decode(latent).sample\n",
    "        reconstructed = (reconstructed + 1) / 2\n",
    "        for _ in range(iter-1):\n",
    "            latent = self.vae.encode(2*reconstructed-1).latent_dist.sample()\n",
    "            reconstructed = self.vae.decode(latent).sample\n",
    "            reconstructed = (reconstructed + 1) / 2\n",
    "        return reconstructed\n",
    "\n",
    "    def embed_watermark(self, original: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed watermark in image using latent frequency space optimization\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor [B, C, H, W]\n",
    "            message: Binary message {-1, 1} [B, message_bits]\n",
    "        \n",
    "        Returns:\n",
    "            Watermarked image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        image = original.to(self.args.device)\n",
    "        # message = message.to(self.device)\n",
    "\n",
    "        # prompt_embeds = self.pipe._encode_prompt([\"\"], self.args.device, 1, True, None)\n",
    "        self.pipe.scheduler.set_timesteps(self.args.num_inference_steps, device=self.args.device)\n",
    "        timesteps = self.pipe.scheduler.timesteps\n",
    "        target_steps = timesteps[-40:]\n",
    "        \n",
    "        generator = torch.Generator(device=\"cpu\").manual_seed(self.args.seed)\n",
    "        noise_generator = torch.Generator(device=self.args.device).manual_seed(self.args.seed)\n",
    "\n",
    "        # Step 1: Encode image to latent space\n",
    "        # image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "        latent = self.pipe.vae.encode(2*image-1).latent_dist.sample(generator=noise_generator) # [-1, 1], [B,4,64,64]\n",
    "        latent = latent * self.pipe.vae.config.scaling_factor\n",
    "\n",
    "        # Step 2: Transform to frequency domain\n",
    "        latent_fft = torch.fft.fft2(latent, dim=(-2, -1)).detach()\n",
    "        \n",
    "        # Step 3: Initialize perturbation (trainable parameter)\n",
    "        delta_m = torch.zeros_like(latent_fft, requires_grad=True)\n",
    "        optimizer = optim.Adam([delta_m], lr=self.args.lr)\n",
    "\n",
    "        epsilon = torch.randn(latent.shape, generator=noise_generator, device=self.args.device, dtype=latent.dtype).detach()\n",
    "        epsilon_prime = torch.randn(latent.shape, generator=noise_generator, device=self.args.device, dtype=latent.dtype).detach()\n",
    "        \n",
    "        # Training loop\n",
    "        # for step in range(self.args.steps):\n",
    "        for step in tqdm(range(self.args.steps), desc=\"Embedding Watermark\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = self._create_random_mask(image, num_masks=1, mask_percentage=self.args.mask_percentage)\n",
    "            if random.random() < 0.5:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            perturbed_fft = latent_fft + delta_m\n",
    "            watermarked_latent = torch.fft.ifft2(perturbed_fft, dim=(-2, -1)).real\n",
    "            watermarked_latent_vis = watermarked_latent / self.pipe.vae.config.scaling_factor\n",
    "            watermarked_image = self.pipe.vae.decode(watermarked_latent_vis).sample\n",
    "            watermarked_image = (watermarked_image + 1) / 2\n",
    "\n",
    "            masked = watermarked_image * mask + (1 - mask) * image\n",
    "\n",
    "            # watermarked_latent = watermarked_latent * self.pipe.vae.config.scaling_factor\n",
    "            \n",
    "            inpaint_mask_latent = F.interpolate(1-mask, size=(latent.shape[2], latent.shape[3]))\n",
    "            inpaint_mask_latent = (inpaint_mask_latent > 0.65).to(dtype=latent.dtype)\n",
    "\n",
    "            # start_index = len(timesteps) - 3\n",
    "            # end_index = len(timesteps) - 1\n",
    "            # random_index = torch.randint(start_index, end_index, (1,), generator=generator).item()\n",
    "            # target_timestep = timesteps[random_index]\n",
    "            \n",
    "            # with torch.no_grad():\n",
    "            #     epsilon = torch.randn(watermarked_latent.shape, generator=noise_generator, device=self.args.device, dtype=watermarked_latent.dtype)\n",
    "            #     epsilon_prime = torch.randn(watermarked_latent.shape, generator=noise_generator, device=self.args.device, dtype=watermarked_latent.dtype)\n",
    "            current_latent = self.pipe.scheduler.add_noise(watermarked_latent, epsilon, target_steps[0])\n",
    "\n",
    "            for t in target_steps:\n",
    "                simulated_noise_pred = epsilon * (1 - inpaint_mask_latent) + epsilon_prime * inpaint_mask_latent\n",
    "                # current_latent = self.pipe.scheduler.add_noise(simulated_noise_pred, t, current_latent)\n",
    "                x_prev = self.pipe.scheduler.step(simulated_noise_pred, t, current_latent, return_dict=False)[0]\n",
    "                current_latent = x_prev\n",
    "\n",
    "            attacked_latent = current_latent\n",
    "\n",
    "            img_edit = self.pipe.vae.decode(1 / self.pipe.vae.config.scaling_factor * attacked_latent).sample\n",
    "            img_edit = (img_edit + 1) / 2\n",
    "            img_edit = img_edit.clamp(0, 1)\n",
    "\n",
    "            img_edit_loc = norm_dino(img_edit)\n",
    "            masked_loc = norm_dino(masked)\n",
    "            # masked_1 = norm_dino(masked_1)\n",
    "\n",
    "            # k = self.args.dino_image_size//self.args.grid_size\n",
    "            # k = int(self.args.vae_image_size // 8)\n",
    "            # mask_down = F.interpolate(mask, size=(k, k), mode=\"bilinear\", align_corners=False)\n",
    "            # gt_mask_patch = gt_mask_patch.view(1, self.args.grid_size*self.args.grid_size, 1)\n",
    "            # gt_mask_patch = F.avg_pool2d(mask, kernel_size=k, stride=k).view(1, self.args.grid_size*self.args.grid_size, 1)\n",
    "\n",
    "            loss_m = self._mask_loss(img_edit_loc, mask)\n",
    "            loss_d = self._dice_loss(img_edit_loc, mask)\n",
    "            loss_m1 = self._mask_loss(masked_loc, mask)\n",
    "            loss_d1 = self._dice_loss(masked_loc, mask)\n",
    "            # loss_auth = self._absolute_auth_loss(masked, gt_mask_patch)\n",
    "            # loss_auth1 = self._absolute_auth_loss(masked_1, gt_mask_patch)\n",
    "\n",
    "            ## related to secret key\n",
    "            # k = self.args.dino_image_size//self.args.grid_size\n",
    "            # gt_mask_patch = F.avg_pool2d(mask, kernel_size=k, stride=k).view(1, self.args.grid_size*self.args.grid_size, 1)\n",
    "\n",
    "            # # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0]\n",
    "            # features = self.image_encoder(watermarked_image)[1] # [B, 384, 14, 14]\n",
    "            # B, C, H, W = features.shape\n",
    "            # features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            # dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "            # # loss_auth = self._auth_loss(dot_products, self.secret_key, gt_mask_patch)\n",
    "            # loss_auth = self._absolute_auth_loss(dot_products, gt_mask_patch)\n",
    "            \n",
    "            # # features_1 = self.image_encoder.get_intermediate_layers(masked_1)[0]\n",
    "            # features_1 = self.image_encoder(masked_1)[1]\n",
    "            # B, C, H, W = features_1.shape\n",
    "            # features_1 = features_1.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            # dot_products_1 = torch.matmul(features_1, self.direction_vectors.T)\n",
    "            # # loss_auth_1 = self._auth_loss(dot_products_1, self.secret_key, gt_mask_patch)\n",
    "            # loss_auth_1 = self._absolute_auth_loss(dot_products_1, gt_mask_patch)\n",
    "\n",
    "            # watermarked_image = denorm_dino(watermarked_image)\n",
    "            # masked = denorm_dino(masked)\n",
    "            # masked_1 = denorm_dino(masked_1)\n",
    "            # img_edit_loc = denorm_dino(img_edit_loc)\n",
    "\n",
    "            loss_psnr = self._psnr_loss(watermarked_image, image)\n",
    "            loss_lpips = self._lpips_loss(watermarked_image, image)\n",
    "\n",
    "            # total_loss = (loss_m + loss_m1 + # loss_m2 + #loss_m3 + \n",
    "            #     loss_d + loss_d1 +\n",
    "            #     self.args.lambda_p * loss_psnr + \n",
    "            #     self.args.lambda_i * loss_lpips)\n",
    "\n",
    "            # if step < 200:\n",
    "            #     auth_loss_weight = 1.0\n",
    "            #     shape_loss_weight = 0.0\n",
    "            # else:  # Stage 2: Key Embedding\n",
    "            #     auth_loss_weight = 1.0 \n",
    "            #     shape_loss_weight = 1.0\n",
    "\n",
    "            # auth_loss_weight = 0.5\n",
    "            # shape_loss_weight = 1.0\n",
    "\n",
    "            # Combined loss (Equation 10 from paper)\n",
    "            # total_loss = auth_loss_weight * (loss_auth + loss_auth_1) + \\\n",
    "            #              shape_loss_weight * (loss_m + loss_m1 + loss_d + loss_d1) + \\\n",
    "            #              self.args.lambda_p * loss_psnr + \\\n",
    "            #              self.args.lambda_i * loss_lpips\n",
    "\n",
    "            clean_weight = 1.0\n",
    "            noisy_weight = 1.0\n",
    "            \n",
    "            total_loss = noisy_weight * (loss_m + loss_d) + \\\n",
    "                         clean_weight * (loss_m1 + loss_d1) + \\\n",
    "                         self.args.lambda_p * loss_psnr + \\\n",
    "                         self.args.lambda_i * loss_lpips\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step == 0 or (step+1) % 100 == 0:\n",
    "                psnr_val = self._compute_psnr(watermarked_image.detach(), image.detach())\n",
    "                print(f\"Step {step+1}, Loss: {total_loss.item():.4f}, PSNR: {psnr_val:.2f}\")\n",
    "                print(f\"Mask Loss: {loss_m.item():.4f}, DICE Loss: {loss_d.item():.4f}\")\n",
    "                print(f\"Mask1 Loss: {(loss_m1).item():.4f}, DICE1 Loss: {loss_d1.item():.4f}\")\n",
    "                # print(f\"Auth Loss: {(loss_auth).item():.4f}, Auth1 Loss: {loss_auth1.item():.4f}\")\n",
    "                print(f\"PSNR Loss: {loss_psnr.item():.4f}, LPIPS Loss: {loss_lpips.item():.4f}\")\n",
    "\n",
    "                # # Save images for analysis\n",
    "                torchvision.utils.save_image(watermarked_image.detach(), os.path.join(args.output_dir, f\"analysis_dist_wm_step{step+1}.png\"))\n",
    "                torchvision.utils.save_image(img_edit.detach(), os.path.join(args.output_dir, f\"analysis_dist_inpaint_step{step+1}.png\"))\n",
    "                # # torchvision.utils.save_image(masked.detach(), os.path.join(args.output_dir, f\"analysis_dist_masked_step{step+1}.png\"))\n",
    "                # # torchvision.utils.save_image(masked_1.detach(), os.path.join(args.output_dir, f\"analysis_dist_masked1_step{step+1}.png\"))\n",
    "\n",
    "                sig_w = torch.sigmoid(self.decode_watermark(watermarked_image.detach())).cpu().numpy().flatten()\n",
    "                sig_e = torch.sigmoid(self.decode_watermark(img_edit.detach())).cpu().numpy().flatten()\n",
    "                # # sig_m = torch.sigmoid(self.decode_watermark(masked.detach())).cpu().numpy().flatten()\n",
    "                # # sig_m1 = torch.sigmoid(self.decode_watermark(masked_1.detach())).cpu().numpy().flatten()\n",
    "\n",
    "                print(f\"[A: Watermarked] Mean: {sig_w.mean():.2f}, Std: {sig_w.std():.2f}, Min: {sig_w.min():.2f}, Max: {sig_w.max():.2f}\")\n",
    "                print(f\"[B: Inpaint] Mean: {sig_e.mean():.2f}, Std: {sig_e.std():.2f}, Min: {sig_e.min():.2f}, Max: {sig_e.max():.2f}\")\n",
    "                # print(f\"[B: Spliced] Mean: {sig_m.mean():.2f}, Std: {sig_m.std():.2f}, Min: {sig_m.min():.2f}, Max: {sig_m.max():.2f}\")\n",
    "                # print(f\"[C: Noisy Spliced] Mean: {sig_m1.mean():.2f}, Std: {sig_m1.std():.2f}, Min: {sig_m1.min():.2f}, Max: {sig_m1.max():.2f}\")\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.hist(sig_w, bins=50, alpha=0.5, label='A: Watermarked')\n",
    "                plt.hist(sig_e, bins=50, alpha=0.5, label='B: Inpaint')\n",
    "                # plt.hist(sig_m, bins=50, alpha=0.5, label='B: Spliced')\n",
    "                # plt.hist(sig_m1, bins=50, alpha=0.5, label='C: Noisy Spliced')\n",
    "                plt.title(f'Logit Distribution Comparison: Step {step+1}')\n",
    "                plt.xlabel('Logit Value')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.savefig(os.path.join(args.output_dir, f\"analysis_dist_step{step+1}.png\"), bbox_inches='tight')\n",
    "\n",
    "        # Final watermarked image\n",
    "        final_fft = latent_fft + delta_m\n",
    "        final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "        final_watermarked = self.pipe.vae.decode(1 / self.pipe.vae.config.scaling_factor * final_latent).sample\n",
    "        final_watermarked = (final_watermarked + 1) / 2\n",
    "        final_watermarked = final_watermarked.clamp(0, 1)\n",
    "        \n",
    "        return final_watermarked.detach()\n",
    "    \n",
    "    def decode_watermark(self, watermarked_image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode watermark from image using pre-trained image encoder\n",
    "        \n",
    "        Args:\n",
    "            watermarked_image: Watermarked image tensor [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Decoded message {-1, 1} [B, message_bits]\n",
    "        \"\"\"\n",
    "        watermarked_image = watermarked_image.to(self.args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Extract features using image encoder\n",
    "            # features = self.image_encoder(watermarked_image) # [1, 256, 384]\n",
    "            watermarked_image = norm_dino(watermarked_image) \n",
    "            # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0] # [1, 256, 384]\n",
    "            features = self.image_encoder(watermarked_image)[1]\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            # Compute dot products with direction vectors\n",
    "            dot_products = torch.matmul(features, self.direction_vectors.T) # [1, 256, 384]*[1, 384, 256] -> [1, 256, 1]\n",
    "            # agreement_scores = dot_products * self.secret_key\n",
    "\n",
    "            B = dot_products.shape[0]\n",
    "            H = W = int(dot_products.shape[1] ** 0.5)\n",
    "            grid = dot_products.view(B, H, W).unsqueeze(0) # [1, 256, 1] -> [1, 1, 16, 16]\n",
    "            grid = F.interpolate(grid, size=self.args.vae_image_size, mode='bilinear', align_corners=False)\n",
    "        return grid\n",
    "    \n",
    "    def _message_loss(self, watermarked_image: torch.Tensor, message: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Hinge loss for message embedding (Equation 7)\"\"\"\n",
    "        features = self.image_encoder(watermarked_image)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        \n",
    "        # Hinge loss with margin\n",
    "        projections = dot_products * message\n",
    "        loss = torch.clamp(self.mu - projections, min=0).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _mask_loss(self, watermarked_image: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss based on patch-wise watermark detection to enforce a global watermark presence.\n",
    "        The ground truth mask is implicitly all-ones, meaning the loss is minimized when all patches\n",
    "        correctly embed the watermark.\n",
    "        \"\"\"\n",
    "        # image_for_dino = F.interpolate(watermarked_image, \n",
    "        #                                size=(self.args.dino_image_size, self.args.dino_image_size), \n",
    "        #                                mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0] # [B, Num_Patches, Feature_Dim]\n",
    "        features = self.image_encoder(watermarked_image)[1]\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        # agreement_scores = dot_products * self.secret_key\n",
    "        B = dot_products.shape[0]\n",
    "        H = W = int(dot_products.shape[1] ** 0.5)\n",
    "        grid = dot_products.view(B, H, W).unsqueeze(0)\n",
    "        # grid = dot_products.view(self.args.grid_size, self.args.grid_size).unsqueeze(0).unsqueeze(0) # [1, 256, 1] -> [1, 1, 14, 14]\n",
    "        grid = F.interpolate(grid, size=self.args.vae_image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        loss = F.binary_cross_entropy_with_logits(grid, gt_mask)\n",
    "        return loss\n",
    "    \n",
    "    def _psnr_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative PSNR loss (Equation 5)\"\"\"\n",
    "        mse = F.mse_loss(pred, target)\n",
    "        psnr = -10 * torch.log10(mse + 1e-8)\n",
    "        return -psnr  # Negative for minimization\n",
    "    \n",
    "    def _lpips_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simplified LPIPS-like loss\"\"\"\n",
    "        # Simplified perceptual loss using L2 in feature space\n",
    "        pred_gray = 0.299 * pred[:, 0] + 0.587 * pred[:, 1] + 0.114 * pred[:, 2]\n",
    "        target_gray = 0.299 * target[:, 0] + 0.587 * target[:, 1] + 0.114 * target[:, 2]\n",
    "        return F.mse_loss(pred_gray, target_gray)\n",
    "    \n",
    "    def _compute_psnr(self, pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"Compute PSNR between images\"\"\"\n",
    "        mse = F.mse_loss(pred, target).item()\n",
    "        if mse == 0:\n",
    "            return 100.0\n",
    "        return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "    \n",
    "    def _dice_loss(self, watermarked_image, gt_mask, smooth=1e-5):\n",
    "        # image_for_dino = F.interpolate(watermarked_image, \n",
    "        #                                size=(self.args.dino_image_size, self.args.dino_image_size), \n",
    "        #                                mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # features = self.image_encoder.get_intermediate_layers(image_for_dino)[0] # [B, Num_Patches, Feature_Dim]\n",
    "        features = self.image_encoder(watermarked_image)[1]\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        # agreement_scores = dot_products * self.secret_key\n",
    "        B = dot_products.shape[0]\n",
    "        H = W = int(dot_products.shape[1] ** 0.5)\n",
    "        grid = dot_products.view(B, H, W).unsqueeze(0)\n",
    "        # grid = dot_products.view(self.args.grid_size, self.args.grid_size).unsqueeze(0).unsqueeze(0) # [1, 256, 1] -> [1, 1, 14, 14]\n",
    "        grid = F.interpolate(grid, size=self.args.vae_image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        \n",
    "        pred = torch.sigmoid(grid) # Logits to probabilities\n",
    "\n",
    "        # Flatten label and prediction tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = gt_mask.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        \n",
    "        return 1 - dice_coeff\n",
    "\n",
    "    # def _auth_loss(self, dot_products, secret_key, gt_mask, margin=1.5):\n",
    "    #     projections = dot_products * secret_key\n",
    "    #     loss = torch.clamp(margin - projections, min=0)\n",
    "    #     loss = (loss * gt_mask).mean()\n",
    "    #     return loss\n",
    "    \n",
    "    # def _auth_loss(self, dot_products, secret_key, gt_mask):\n",
    "    #     TARGET_SCORE = 5.0\n",
    "    #     target_scores = secret_key * TARGET_SCORE\n",
    "    #     loss = F.mse_loss(dot_products, target_scores, reduction='none')\n",
    "    #     # loss = F.l1_loss(dot_products, target_scores, reduction='none')\n",
    "    #     loss = (loss * gt_mask).mean()\n",
    "    #     return loss\n",
    "    \n",
    "    def _absolute_auth_loss(self, image, gt_mask, TARGET_SCORE = 3.0):\n",
    "        features = self.image_encoder(image)[1]\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        target_map = gt_mask * TARGET_SCORE + (1 - gt_mask) * (-1 * TARGET_SCORE)\n",
    "        loss = F.l1_loss(dot_products, target_map)\n",
    "        return loss\n",
    "    \n",
    "    def compute_bit_accuracy(self, original_message: torch.Tensor, \n",
    "                           decoded_message: torch.Tensor) -> float:\n",
    "        \"\"\"Compute bit accuracy between original and decoded messages\"\"\"\n",
    "        matches = (original_message == decoded_message).float()\n",
    "        return matches.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bf3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = F.mse_loss(a, b).item()\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse)))\n",
    "\n",
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    # Ensure masks are binary\n",
    "    # pred_mask_bin = (pred_mask < 0).float()\n",
    "    pred_mask_bin = torch.sigmoid(pred_mask)\n",
    "    pred_mask_bin = (pred_mask_bin > 0.65).float() # Thresholding at 0.65\n",
    "    gt_mask_bin = (gt_mask > 0).float() # Ground truth might not be 0/1\n",
    "\n",
    "    save_image(pred_mask, \"pred.png\")\n",
    "    save_image(pred_mask_bin, \"pred_bin.png\")\n",
    "    save_image(gt_mask_bin, \"gt.png\")\n",
    "    save_image(pred_mask_bin * gt_mask_bin, \"intersection.png\")\n",
    "    save_image(pred_mask_bin + gt_mask_bin, \"union.png\")\n",
    "\n",
    "    # Intersection and Union\n",
    "    intersection = (pred_mask_bin * gt_mask_bin).sum()\n",
    "    union = (pred_mask_bin + gt_mask_bin).sum() - intersection\n",
    "\n",
    "    iou = intersection / (union + 1e-6) # Add epsilon to avoid division by zero\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/0002.png\"\n",
    "img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/0002.png\"\n",
    "seed = 42\n",
    "proportion_masked = 0.3\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7288e47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee29904d9094191bbb3d0516573f386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef470647e0a42cbb686a3ca2d839b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51be2d54cf644d6b19facd099da07c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 27.41, IoU: 0.5841\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a642e4506b41eaba39b61a0da6d1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 27.41, IoU: 0.6125\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f448d61d2e5c40ce9639d004879cd96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 27.41, IoU: 0.5553\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b054dcfaa2a64065b3d8b67f1c4863ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 27.41, IoU: 0.5874\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e36cc5bc814de88a0d985248b64dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 27.41, IoU: 0.6072\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"sd-legacy/stable-diffusion-inpainting\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    cache_dir='/mnt/nas5/suhyeon/caches'\n",
    ").to(device)\n",
    "\n",
    "args = Params()\n",
    "freqmark = FreqMark(args=args)\n",
    "\n",
    "# secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# print(freqmark.direction_vectors)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "watermarked = load_img(img_path, transforms=args.transform)\n",
    "original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "# original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "# watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "psnrs = []\n",
    "ious = []\n",
    "logits = []\n",
    "\n",
    "for _ in range(trials):\n",
    "    mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "    img_norm, min_norm, max_norm = norm_tensor(watermarked)\n",
    "    img_edit_pil = pipe(prompt=\"\", image=img_norm, mask_image=mask, generator=generator).images[0]\n",
    "    img_edit = to_tensor(img_edit_pil)\n",
    "    img_edit = img_edit.unsqueeze(0).to(device)\n",
    "\n",
    "    img_edit = denorm_tensor(img_edit, min_norm, max_norm)  # [1, 3, H, W]\n",
    "    # img_edit = img_edit * mask + watermarked * (1-mask)\n",
    "\n",
    "    # img_edit = F.interpolate(img_edit, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "    decoded_batch = freqmark.decode_watermark(img_edit)\n",
    "\n",
    "    save_image(img_edit, \"edited.png\")\n",
    "    # original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    # watermarked_224 = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    # mask_224 = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    psnrs.append(compute_psnr(watermarked, original))\n",
    "    ious.append(calculate_iou(decoded_batch, 1-mask))\n",
    "    logits.append(decoded_batch)\n",
    "\n",
    "    print(f\"PSNR: {psnrs[-1]:.2f}, IoU: {ious[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6fe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = Params()\n",
    "# freqmark = FreqMark(args=args)\n",
    "\n",
    "# # secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# # freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# # print(freqmark.direction_vectors)\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# generator = torch.Generator(device=device).manual_seed(seed)\n",
    "# to_tensor = transforms.ToTensor()\n",
    "\n",
    "# watermarked = load_img(img_path, transforms=args.transform)\n",
    "# original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "# original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "# watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# psnrs = []\n",
    "# ious = []\n",
    "# logits = []\n",
    "\n",
    "# for _ in range(trials):\n",
    "#     mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "#     delta = torch.load('delta_m.pt').to(device)\n",
    "\n",
    "#     original_norm = torch.linalg.norm(delta)\n",
    "\n",
    "#     # 2. '구조 없는' 워터마크 생성\n",
    "#     delta_m_random = torch.randn_like(delta)\n",
    "#     random_norm = torch.linalg.norm(delta_m_random)\n",
    "#     delta_m_random = delta_m_random * (original_norm / random_norm) # 세기를 동일하게 맞춤\n",
    "\n",
    "#     # 3. '세기만 약한' 워터마크 생성\n",
    "#     delta_m_scaled = delta * 0.5\n",
    "    \n",
    "#     results = {}\n",
    "\n",
    "#     latent = freqmark.vae.encode(2*original-1).latent_dist.sample()\n",
    "#     latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "\n",
    "#     for name, delta in [(\"Optimized\", delta), \n",
    "#                         (\"Random\", delta_m_random), \n",
    "#                         (\"Scaled\", delta_m_scaled)]:\n",
    "        \n",
    "#         final_fft = latent_fft + delta\n",
    "#         final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "#         watermarked_image = (freqmark.vae.decode(final_latent).sample + 1) / 2\n",
    "        \n",
    "#         watermarked_image = F.interpolate(watermarked_image, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "#         logits = freqmark.decode_watermark(watermarked_image) \n",
    "        \n",
    "#         # 워터마크가 있어야 할 영역(gt_mask=1)의 평균 logit 점수 계산\n",
    "#         # avg_logit = (logits * gt_mask_resized).sum() / gt_mask_resized.sum()\n",
    "#         # results[name] = avg_logit.item()\n",
    "#         save_image(logits, f\"logits_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c890fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Average on 5 trials ##\n",
      "PSNR (imperceptibility): 27.41 dB\n",
      "IoU (localization accuracy): 0.5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"## Average on {trials} trials ##\")\n",
    "print(f\"PSNR (imperceptibility): {np.mean(psnrs):.2f} dB\")\n",
    "print(f\"IoU (localization accuracy): {np.mean(ious):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bad2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "# watermarked = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "# mask = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "save_image(original, \"eval_original.png\")\n",
    "save_image(watermarked, \"eval_watermarked.png\")\n",
    "save_image(img_edit, \"eval_edited_w_mask.png\")\n",
    "save_image(decoded_batch, \"eval_localized.png\")\n",
    "save_image(1-mask, \"eval_mask.png\")\n",
    "save_image(torch.abs(img_edit-original)*10, \"eval_edit-ori.png\")\n",
    "save_image(torch.abs(watermarked-original)*10, \"eval_wm-ori.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84742381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_logits = torch.cat(logits, dim=0)\n",
    "# # torch.save(total_logits, \"logits_w_l1_loss.pt\")\n",
    "# total_logits = total_logits.cpu().numpy().flatten()\n",
    "# print(f\"Mean: {total_logits.mean():.2f}, Std: {total_logits.std():.2f}, Min: {total_logits.min():.2f}, Max: {total_logits.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = torch.sigmoid(torch.cat(logits, dim=0)).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c50d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1451074/1549856749.py:7: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sig, bins=50, alpha=0.7)#, label='A: w/ L1 loss')\n",
    "plt.title('Logit Distribution Comparison')\n",
    "plt.xlabel('Logit Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('logits_comparison.png')\n",
    "# print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04f96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_b = torch.load(\"logits_w_l1_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_c = total_logits\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {logits_a.mean():.2f}, Std: {logits_a.std():.2f}, Min: {logits_a.min():.2f}, Max: {logits_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {logits_b.mean():.2f}, Std: {logits_b.std():.2f}, Min: {logits_b.min():.2f}, Max: {logits_b.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss (Dual)] Mean: {logits_c.mean():.2f}, Std: {logits_c.std():.2f}, Min: {logits_c.min():.2f}, Max: {logits_c.max():.2f}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(logits_a, bins=50, alpha=0.4, label='A: w/o L')\n",
    "# plt.hist(logits_b, bins=50, alpha=0.4, label='B: w/ L')\n",
    "# plt.hist(logits_c, bins=50, alpha=0.4, label='B: w/ L (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c61381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_a = torch.sigmoid(torch.load(\"logits_wo_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_b = torch.sigmoid(torch.load(\"logits_w_l1_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_c = torch.sigmoid(torch.load(\"logits_w_l1_loss_dual.pt\")).cpu().numpy().flatten()\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {sig_a.mean():.2f}, Std: {sig_a.std():.2f}, Min: {sig_a.min():.2f}, Max: {sig_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {sig_b.mean():.2f}, Std: {sig_b.std():.2f}, Min: {sig_b.min():.2f}, Max: {sig_b.max():.2f}\")\n",
    "# print(f\"[C: w/ L1 Loss (Dual)] Mean: {sig_c.mean():.2f}, Std: {sig_c.std():.2f}, Min: {sig_c.min():.2f}, Max: {sig_c.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d08685",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2196290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(sig_a, bins=50, alpha=0.7, label='A: w/o Add. Loss')\n",
    "# plt.hist(sig_b, bins=50, alpha=0.7, label='B: w/ L1 Loss')\n",
    "# plt.hist(sig_c, bins=50, alpha=0.7, label='B: w/ L1 Loss (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n",
    "# # print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02533add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca09af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
