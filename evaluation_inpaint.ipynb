{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee43cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, AutoencoderKL\n",
    "import timm\n",
    "\n",
    "class Denormalize(transforms.Normalize):\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.tensor(mean)\n",
    "        std = torch.tensor(std)\n",
    "        self.mean_rev = -mean / std\n",
    "        self.std_rev = 1 / std\n",
    "        super().__init__(mean=self.mean_rev, std=self.std_rev)\n",
    "\n",
    "norm_dino = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "denorm_dino = Denormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize_img,\n",
    "])\n",
    "\n",
    "def load_img(path, transforms=None):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = transforms(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def norm_tensor(tensor):\n",
    "    t = tensor.clone().detach()\n",
    "    \n",
    "    min_val = t.min()\n",
    "    max_val = t.max()\n",
    "\n",
    "    tensor_norm = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    print(f\"Tensor normalized: min={tensor_norm.min()}, max={tensor_norm.max()}\")\n",
    "    \n",
    "    return tensor_norm, min_val, max_val\n",
    "\n",
    "def denorm_tensor(tensor, original_min=None, original_max=None):\n",
    "    t = tensor.clone().detach()\n",
    "\n",
    "    return t * (original_max - original_min) + original_min\n",
    "\n",
    "def create_random_mask(img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "    _, _, height, width = img_pt.shape\n",
    "    mask_area = int(height * width * mask_percentage)\n",
    "    masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "    if mask_percentage >= 0.999:\n",
    "        # Full mask for entire image\n",
    "        return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "    for ii in range(num_masks):\n",
    "        placed = False\n",
    "        attempts = 0\n",
    "        while not placed and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            max_dim = int(mask_area ** 0.5)\n",
    "            mask_width = random.randint(1, max_dim)\n",
    "            mask_height = mask_area // mask_width\n",
    "\n",
    "            # Allow broader aspect ratios for larger masks\n",
    "            aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "            if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                if mask_height <= height and mask_width <= width:\n",
    "                    x_start = random.randint(0, width - mask_width)\n",
    "                    y_start = random.randint(0, height - mask_height)\n",
    "                    overlap = False\n",
    "                    for jj in range(ii):\n",
    "                        if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    if not overlap:\n",
    "                        masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                        placed = True\n",
    "\n",
    "        if not placed:\n",
    "            # Fallback: just fill a central region if all attempts fail\n",
    "            print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "            center_h = height // 2\n",
    "            center_w = width // 2\n",
    "            half_area = int((mask_area // 2) ** 0.5)\n",
    "            h_half = min(center_h, half_area)\n",
    "            w_half = min(center_w, half_area)\n",
    "            masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "    return masks.to(img_pt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ad4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Hyperparameters and configuration settings for FreqMark.\"\"\"\n",
    "    def __init__(self):\n",
    "        # --- System & Paths ---\n",
    "        self.device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "        self.image_path = '/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png'\n",
    "\n",
    "        # --- Model Configurations ---\n",
    "        self.vae_model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "        self.vae_subfolder = \"vae\"\n",
    "        self.dino_model_repo = 'facebookresearch/dinov2'\n",
    "        self.dino_model_name = 'dinov2_vits14'\n",
    "        \n",
    "        # --- Image Size Parameters ---\n",
    "        self.vae_image_size = 512\n",
    "        self.dino_image_size = 224\n",
    "        self.transform = transforms.Compose([\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # --- FreqMark Core Parameters ---\n",
    "        self.message_bits = 2\n",
    "        self.feature_dim = 192\n",
    "        self.margin = 1.0\n",
    "        self.grid_size = 28\n",
    "        self.num_patches = self.grid_size*self.grid_size\n",
    "\n",
    "        # --- Optimization Parameters ---\n",
    "        self.lr = 2.0\n",
    "        self.steps = 400\n",
    "        self.lambda_p = 0.05\n",
    "        self.lambda_i = 0.25\n",
    "\n",
    "        # --- Robustness Parameters ---\n",
    "        self.eps1_std = 0.25 \n",
    "        self.eps2_std = 0.06\n",
    "        \n",
    "        # --- Demo/Evaluation Parameters ---\n",
    "        self.batch_size = 4\n",
    "        self.num_test_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqMark:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # Initialize networks\n",
    "        self.vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"vae\").to(self.args.device)\n",
    "        # self.image_encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(self.args.device)\n",
    "        self.image_encoder = timm.create_model(\n",
    "            'convnext_tiny',\n",
    "            pretrained=True,\n",
    "            features_only=True,\n",
    "        ).to(self.args.device)\n",
    "        # Freeze all networks\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Pre-define direction vectors\n",
    "        # self.direction_vectors = torch.load('./sensitive_vec.pt')\n",
    "        # self.direction_vectors = torch.randn(self.args.feature_dim)\n",
    "        # self.direction_vectors = F.normalize(self.direction_vectors, p=2, dim=0)\n",
    "        # self.direction_vectors = self.direction_vectors.unsqueeze(0).to(self.args.device)\n",
    "        # torch.save(self.direction_vectors, './random_vec.pt')\n",
    "        self.direction_vectors = torch.load('./random_vec.pt').to(self.args.device)\n",
    "\n",
    "    \n",
    "        self.mu = self.args.margin      # Hinge loss margin\n",
    "        # self.num_patches = (self.args.dino_image_size // 14) ** 2\n",
    "        # self.secret_key = torch.randint(0, 2, (1, self.num_patches, 1), device=self.args.device) * 2 - 1 # {-1, 1}\n",
    "        # self.secret_key = torch.randint(0, 2, (1, args.grid_size*args.grid_size, 1), device=self.args.device) * 2 - 1 # {-1, 1}\n",
    "\n",
    "        # torch.save(self.secret_key, './secret_key.pt')\n",
    "        # self.secret_key = -1 * torch.ones((1, args.grid_size*args.grid_size, 1), dtype=torch.int, device=self.args.device)\n",
    "        # self.secret_key = torch.load('./secret_key.pt').to(self.args.device)\n",
    "\n",
    "\n",
    "    # def _init_direction_vectors(self) -> torch.Tensor:\n",
    "    #     \"\"\"Initialize direction vectors as described in paper\"\"\"\n",
    "    #     # binary bit for each patch\n",
    "    #     # vectors = torch.zeros(1, self.args.feature_dim)\n",
    "    #     # for i in range(1):\n",
    "    #     #     vectors[i, self.args.feature_dim-1] = 1.0  # One-hot encoding\n",
    "    #     # print(vectors)\n",
    "    #     # return vectors.to(self.args.device)\n",
    "    #     # random_vector = torch.randn(self.args.feature_dim, device=self.args.device)\n",
    "    #     # normalized_vector = F.normalize(random_vector, p=2, dim=0)\n",
    "    #     # return normalized_vector.unsqueeze(0)\n",
    "    #     self.direction_vectors = torch.load('./insensitive_vec.pt')\n",
    "\n",
    "    def _create_random_mask(self, img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "        _, _, height, width = img_pt.shape\n",
    "        mask_area = int(height * width * mask_percentage)\n",
    "        masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "        if mask_percentage >= 0.999:\n",
    "            # Full mask for entire image\n",
    "            return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "        for ii in range(num_masks):\n",
    "            placed = False\n",
    "            attempts = 0\n",
    "            while not placed and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "\n",
    "                max_dim = int(mask_area ** 0.5)\n",
    "                mask_width = random.randint(1, max_dim)\n",
    "                mask_height = mask_area // mask_width\n",
    "\n",
    "                # Allow broader aspect ratios for larger masks\n",
    "                aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "                if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                    if mask_height <= height and mask_width <= width:\n",
    "                        x_start = random.randint(0, width - mask_width)\n",
    "                        y_start = random.randint(0, height - mask_height)\n",
    "                        overlap = False\n",
    "                        for jj in range(ii):\n",
    "                            if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                                overlap = True\n",
    "                                break\n",
    "                        if not overlap:\n",
    "                            masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                            placed = True\n",
    "\n",
    "            if not placed:\n",
    "                # Fallback: just fill a central region if all attempts fail\n",
    "                print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "                center_h = height // 2\n",
    "                center_w = width // 2\n",
    "                half_area = int((mask_area // 2) ** 0.5)\n",
    "                h_half = min(center_h, half_area)\n",
    "                w_half = min(center_w, half_area)\n",
    "                masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "        return masks.to(img_pt.device)\n",
    "\n",
    "    def vae_recon(self, image: torch.Tensor, iter: int):\n",
    "        \"\"\"VAE reconstruction. Inputs are outputs are 512x512\"\"\"\n",
    "        latent = self.vae.encode(2*image-1).latent_dist.sample()\n",
    "        reconstructed = self.vae.decode(latent).sample\n",
    "        reconstructed = (reconstructed + 1) / 2\n",
    "        for _ in range(iter-1):\n",
    "            latent = self.vae.encode(2*reconstructed-1).latent_dist.sample()\n",
    "            reconstructed = self.vae.decode(latent).sample\n",
    "            reconstructed = (reconstructed + 1) / 2\n",
    "        return reconstructed\n",
    "\n",
    "    def embed_watermark(self, original: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed watermark in image using latent frequency space optimization\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor [B, C, H, W]\n",
    "            message: Binary message {-1, 1} [B, message_bits]\n",
    "        \n",
    "        Returns:\n",
    "            Watermarked image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        original = original.to(self.args.device)\n",
    "        # message = message.to(self.device)\n",
    "        \n",
    "        # Step 1: Encode image to latent space\n",
    "        image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "        latent = self.vae.encode(2*image-1).latent_dist.sample() # [-1, 1], [B,4,64,64]\n",
    "        \n",
    "        # Step 2: Transform to frequency domain\n",
    "        latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "        \n",
    "        # Step 3: Initialize perturbation (trainable parameter)\n",
    "        delta_m = torch.zeros_like(latent_fft, requires_grad=True)\n",
    "        optimizer = optim.Adam([delta_m], lr=self.args.lr)\n",
    "\n",
    "        # Training loop\n",
    "        # for step in range(self.args.steps):\n",
    "        for step in tqdm(range(self.args.steps), desc=\"Embedding Watermark\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = self._create_random_mask(image, num_masks=1, mask_percentage=self.args.mask_percentage)\n",
    "            mask = mask.to(self.args.device)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            perturbed_fft = latent_fft + delta_m\n",
    "            \n",
    "            perturbed_latent = torch.fft.ifft2(perturbed_fft, dim=(-2, -1)).real\n",
    "            \n",
    "            watermarked_image = self.vae.decode(perturbed_latent).sample\n",
    "            watermarked_image = (watermarked_image + 1) / 2\n",
    "            \n",
    "            masked = watermarked_image * mask + (1 - mask) * image\n",
    "\n",
    "            # inpaint\n",
    "            latent_mask = F.interpolate(mask, size=(64, 64), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            std_val_0 = random.uniform(self.args.eps0_std[0], self.args.eps0_std[1])\n",
    "            eps0 = torch.randn_like(perturbed_latent) * std_val_0\n",
    "\n",
    "            perturbed_latent_1 = (perturbed_latent + eps0)*latent_mask + perturbed_latent*(1-latent_mask)\n",
    "\n",
    "            watermarked_image_1 = self.vae.decode(perturbed_latent_1).sample\n",
    "            masked_1 = (watermarked_image_1 + 1) / 2\n",
    "            masked_1 = masked_1 * mask + (1 - mask) * image ################################\n",
    "\n",
    "            # Compute losses\n",
    "            image = F.interpolate(original, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            masked = F.interpolate(masked, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            masked_1 = F.interpolate(masked_1, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            # masked_2 = F.interpolate(masked_2, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            # masked_3 = F.interpolate(masked_3, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            watermarked_image = F.interpolate(watermarked_image, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            watermarked_image = norm_dino(watermarked_image)\n",
    "            masked = norm_dino(masked)\n",
    "            masked_1 = norm_dino(masked_1)\n",
    "\n",
    "            loss_m = self._mask_loss(masked, mask)\n",
    "            loss_d = self._dice_loss(masked, mask)\n",
    "            loss_m1 = self._mask_loss(masked_1, mask)\n",
    "            loss_d1 = self._dice_loss(masked_1, mask)\n",
    "\n",
    "            k = self.args.dino_image_size//self.args.grid_size\n",
    "            gt_mask_patch = F.avg_pool2d(mask, kernel_size=k, stride=k).view(1, self.args.grid_size*self.args.grid_size, 1)\n",
    "\n",
    "            # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0]\n",
    "            features = self.image_encoder(watermarked_image)[1] # [B, 384, 14, 14]\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "            loss_auth = self._auth_loss(dot_products, self.secret_key, gt_mask_patch)\n",
    "            \n",
    "            # features_1 = self.image_encoder.get_intermediate_layers(masked_1)[0]\n",
    "            features_1 = self.image_encoder(masked_1)[1]\n",
    "            B, C, H, W = features_1.shape\n",
    "            features_1 = features_1.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            dot_products_1 = torch.matmul(features_1, self.direction_vectors.T)\n",
    "            loss_auth_1 = self._auth_loss(dot_products_1, self.secret_key, gt_mask_patch)\n",
    "\n",
    "\n",
    "            watermarked_image = denorm_dino(watermarked_image)\n",
    "            masked = denorm_dino(masked)\n",
    "            masked_1 = denorm_dino(masked_1)\n",
    "\n",
    "            loss_psnr = self._psnr_loss(watermarked_image, image)\n",
    "            loss_lpips = self._lpips_loss(watermarked_image, image)\n",
    "\n",
    "            if step < 200:\n",
    "                auth_loss_weight = 1.0\n",
    "                shape_loss_weight = 0.0\n",
    "            else:  # Stage 2: Key Embedding\n",
    "                auth_loss_weight = 1.0\n",
    "                shape_loss_weight = 1.0\n",
    "\n",
    "            # auth_loss_weight = 1.0\n",
    "            # shape_loss_weight = 1.0\n",
    "\n",
    "            # Combined loss (Equation 10 from paper)\n",
    "            total_loss = auth_loss_weight * (loss_auth + loss_auth_1) + \\\n",
    "                         shape_loss_weight * (loss_m + loss_m1 + loss_d + loss_d1) + \\\n",
    "                         self.args.lambda_p * loss_psnr + \\\n",
    "                         self.args.lambda_i * loss_lpips\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step == 0 or (step+1) % 100 == 0:\n",
    "                psnr_val = self._compute_psnr(watermarked_image, image)\n",
    "                print(f\"Step {step+1}, Loss: {total_loss.item():.4f}, PSNR: {psnr_val:.2f}\")\n",
    "                print(f\"Mask Loss: {loss_m.item():.4f}, DICE Loss: {loss_d.item():.4f}\")\n",
    "                print(f\"Mask1 Loss: {(loss_m1).item():.4f}, DICE1 Loss: {loss_d1.item():.4f}\")\n",
    "                print(f\"Auth Loss: {(loss_auth).item():.4f}, Auth1 Loss: {loss_auth_1.item():.4f}\")\n",
    "                print(f\"PSNR Loss: {loss_psnr.item():.4f}, LPIPS Loss: {loss_lpips.item():.4f}\")\n",
    "\n",
    "        # Final watermarked image\n",
    "        final_fft = latent_fft + delta_m\n",
    "        final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "        final_watermarked = self.vae.decode(final_latent).sample\n",
    "        final_watermarked = (final_watermarked + 1) / 2\n",
    "        \n",
    "        return final_watermarked.detach()\n",
    "    \n",
    "    def decode_watermark(self, watermarked_image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode watermark from image using pre-trained image encoder\n",
    "        \n",
    "        Args:\n",
    "            watermarked_image: Watermarked image tensor [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Decoded message {-1, 1} [B, message_bits]\n",
    "        \"\"\"\n",
    "        watermarked_image = watermarked_image.to(self.args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Extract features using image encoder\n",
    "            # features = self.image_encoder(watermarked_image) # [1, 256, 384]\n",
    "            watermarked_image = norm_dino(watermarked_image)\n",
    "            # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0] # [1, 256, 384]\n",
    "            features = self.image_encoder(watermarked_image)[1]\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            # Compute dot products with direction vectors\n",
    "            dot_products = torch.matmul(features, self.direction_vectors.T) # [1, 256, 384]*[1, 384, 256] -> [1, 256, 1]\n",
    "            # agreement_scores = dot_products * self.secret_key\n",
    "\n",
    "            B = dot_products.shape[0]\n",
    "            H = W = int(dot_products.shape[1] ** 0.5)\n",
    "            grid = dot_products.view(B, H, W).unsqueeze(0) # [1, 256, 1] -> [1, 1, 16, 16]\n",
    "            grid = F.interpolate(grid, size=self.args.dino_image_size, mode='bilinear', align_corners=False)\n",
    "        return grid\n",
    "    \n",
    "    def _message_loss(self, watermarked_image: torch.Tensor, message: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Hinge loss for message embedding (Equation 7)\"\"\"\n",
    "        features = self.image_encoder(watermarked_image)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        \n",
    "        # Hinge loss with margin\n",
    "        projections = dot_products * message\n",
    "        loss = torch.clamp(self.mu - projections, min=0).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _mask_loss(self, watermarked_image: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss based on patch-wise watermark detection to enforce a global watermark presence.\n",
    "        The ground truth mask is implicitly all-ones, meaning the loss is minimized when all patches\n",
    "        correctly embed the watermark.\n",
    "        \"\"\"\n",
    "        # image_for_dino = F.interpolate(watermarked_image, \n",
    "        #                                size=(self.args.dino_image_size, self.args.dino_image_size), \n",
    "        #                                mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # features = self.image_encoder.get_intermediate_layers(watermarked_image)[0] # [B, Num_Patches, Feature_Dim]\n",
    "        features = self.image_encoder(watermarked_image)[1]\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        agreement_scores = dot_products * self.secret_key\n",
    "        B = dot_products.shape[0]\n",
    "        H = W = int(dot_products.shape[1] ** 0.5)\n",
    "        grid = agreement_scores.view(B, H, W).unsqueeze(0)\n",
    "        # grid = dot_products.view(self.args.grid_size, self.args.grid_size).unsqueeze(0).unsqueeze(0) # [1, 256, 1] -> [1, 1, 14, 14]\n",
    "        grid = F.interpolate(grid, size=self.args.dino_image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        loss = F.binary_cross_entropy_with_logits(grid, gt_mask)\n",
    "        return loss\n",
    "    \n",
    "    def _psnr_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative PSNR loss (Equation 5)\"\"\"\n",
    "        mse = F.mse_loss(pred, target)\n",
    "        psnr = -10 * torch.log10(mse + 1e-8)\n",
    "        return -psnr  # Negative for minimization\n",
    "    \n",
    "    def _lpips_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simplified LPIPS-like loss\"\"\"\n",
    "        # Simplified perceptual loss using L2 in feature space\n",
    "        pred_gray = 0.299 * pred[:, 0] + 0.587 * pred[:, 1] + 0.114 * pred[:, 2]\n",
    "        target_gray = 0.299 * target[:, 0] + 0.587 * target[:, 1] + 0.114 * target[:, 2]\n",
    "        return F.mse_loss(pred_gray, target_gray)\n",
    "    \n",
    "    def _compute_psnr(self, pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"Compute PSNR between images\"\"\"\n",
    "        mse = F.mse_loss(pred, target).item()\n",
    "        if mse == 0:\n",
    "            return 100.0\n",
    "        return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "    \n",
    "    def _dice_loss(self, watermarked_image, gt_mask, smooth=1e-5):\n",
    "        # image_for_dino = F.interpolate(watermarked_image, \n",
    "        #                                size=(self.args.dino_image_size, self.args.dino_image_size), \n",
    "        #                                mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # features = self.image_encoder.get_intermediate_layers(image_for_dino)[0] # [B, Num_Patches, Feature_Dim]\n",
    "        features = self.image_encoder(watermarked_image)[1]\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        dot_products = torch.matmul(features, self.direction_vectors.T)\n",
    "        agreement_scores = dot_products * self.secret_key\n",
    "        B = dot_products.shape[0]\n",
    "        H = W = int(dot_products.shape[1] ** 0.5)\n",
    "        grid = agreement_scores.view(B, H, W).unsqueeze(0)\n",
    "        # grid = dot_products.view(self.args.grid_size, self.args.grid_size).unsqueeze(0).unsqueeze(0) # [1, 256, 1] -> [1, 1, 14, 14]\n",
    "        grid = F.interpolate(grid, size=self.args.dino_image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        \n",
    "        pred = torch.sigmoid(grid) # Logits to probabilities\n",
    "\n",
    "        # Flatten label and prediction tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = gt_mask.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        \n",
    "        return 1 - dice_coeff\n",
    "\n",
    "    # def _auth_loss(self, dot_products, secret_key, gt_mask, margin=1.5):\n",
    "    #     projections = dot_products * secret_key\n",
    "    #     loss = torch.clamp(margin - projections, min=0)\n",
    "    #     loss = (loss * gt_mask).mean()\n",
    "    #     return loss\n",
    "    \n",
    "    def _auth_loss(self, dot_products, secret_key, gt_mask):\n",
    "        TARGET_SCORE = 4.0\n",
    "        target_scores = secret_key * TARGET_SCORE\n",
    "        # loss = F.mse_loss(dot_products, target_scores, reduction='none')\n",
    "        loss = F.l1_loss(dot_products, target_scores, reduction='none')\n",
    "        loss = (loss * gt_mask).mean()\n",
    "        return loss\n",
    "    \n",
    "    def compute_bit_accuracy(self, original_message: torch.Tensor, \n",
    "                           decoded_message: torch.Tensor) -> float:\n",
    "        \"\"\"Compute bit accuracy between original and decoded messages\"\"\"\n",
    "        matches = (original_message == decoded_message).float()\n",
    "        return matches.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bf3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = F.mse_loss(a, b).item()\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse)))\n",
    "\n",
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    # Ensure masks are binary\n",
    "    # pred_mask_bin = (pred_mask < 0).float()\n",
    "    pred_mask_bin = torch.sigmoid(pred_mask)\n",
    "    pred_mask_bin = (pred_mask_bin > 0.65).float() # Thresholding at 0.65\n",
    "    gt_mask_bin = (gt_mask > 0).float() # Ground truth might not be 0/1\n",
    "\n",
    "    save_image(pred_mask, \"pred.png\")\n",
    "    save_image(pred_mask_bin, \"pred_bin.png\")\n",
    "    save_image(gt_mask_bin, \"gt.png\")\n",
    "    save_image(pred_mask_bin * gt_mask_bin, \"intersection.png\")\n",
    "    save_image(pred_mask_bin + gt_mask_bin, \"union.png\")\n",
    "\n",
    "    # Intersection and Union\n",
    "    intersection = (pred_mask_bin * gt_mask_bin).sum()\n",
    "    union = (pred_mask_bin + gt_mask_bin).sum() - intersection\n",
    "\n",
    "    iou = intersection / (union + 1e-6) # Add epsilon to avoid division by zero\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/0002.png\"\n",
    "img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/analysis_dist_wm_step400.png\"\n",
    "seed = 42\n",
    "proportion_masked = 0.3\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7288e47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c674a5ba8d4f0cbda7e5755df737c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e611724bb9d84fdbb9accc92272a32dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 29.60, IoU: 0.9244\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae32758b786147f3ae0932f7d50edf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 29.60, IoU: 0.9097\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed636e7fc3640b9b93940b6e891b2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 29.60, IoU: 0.9143\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30dc45494d548c7be3a575b423d6ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 29.60, IoU: 0.8957\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b9b18d4c6543ee9ac716047b697fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 29.60, IoU: 0.9151\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"sd-legacy/stable-diffusion-inpainting\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    cache_dir='/mnt/nas5/suhyeon/caches'\n",
    ").to(device)\n",
    "\n",
    "args = Params()\n",
    "freqmark = FreqMark(args=args)\n",
    "\n",
    "# secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# print(freqmark.direction_vectors)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "watermarked = load_img(img_path, transforms=args.transform)\n",
    "original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "psnrs = []\n",
    "ious = []\n",
    "logits = []\n",
    "\n",
    "for _ in range(trials):\n",
    "    mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "    img_norm, min_norm, max_norm = norm_tensor(watermarked)\n",
    "    img_edit_pil = pipe(prompt=\"\", image=img_norm, mask_image=mask, generator=generator).images[0]\n",
    "    img_edit = to_tensor(img_edit_pil)\n",
    "    img_edit = img_edit.unsqueeze(0).to(device)\n",
    "\n",
    "    img_edit = denorm_tensor(img_edit, min_norm, max_norm)  # [1, 3, H, W]\n",
    "    # img_edit = img_edit * mask + watermarked * (1-mask)\n",
    "\n",
    "    img_edit = F.interpolate(img_edit, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "    decoded_batch = freqmark.decode_watermark(img_edit)\n",
    "\n",
    "    save_image(img_edit, \"edited.png\")\n",
    "    original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    watermarked_224 = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    mask_224 = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    psnrs.append(compute_psnr(watermarked_224, original))\n",
    "    ious.append(calculate_iou(decoded_batch, 1-mask_224))\n",
    "    logits.append(decoded_batch)\n",
    "\n",
    "    print(f\"PSNR: {psnrs[-1]:.2f}, IoU: {ious[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6fe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = Params()\n",
    "# freqmark = FreqMark(args=args)\n",
    "\n",
    "# # secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# # freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# # print(freqmark.direction_vectors)\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# generator = torch.Generator(device=device).manual_seed(seed)\n",
    "# to_tensor = transforms.ToTensor()\n",
    "\n",
    "# watermarked = load_img(img_path, transforms=args.transform)\n",
    "# original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "# original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "# watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# psnrs = []\n",
    "# ious = []\n",
    "# logits = []\n",
    "\n",
    "# for _ in range(trials):\n",
    "#     mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "#     delta = torch.load('delta_m.pt').to(device)\n",
    "\n",
    "#     original_norm = torch.linalg.norm(delta)\n",
    "\n",
    "#     # 2. '구조 없는' 워터마크 생성\n",
    "#     delta_m_random = torch.randn_like(delta)\n",
    "#     random_norm = torch.linalg.norm(delta_m_random)\n",
    "#     delta_m_random = delta_m_random * (original_norm / random_norm) # 세기를 동일하게 맞춤\n",
    "\n",
    "#     # 3. '세기만 약한' 워터마크 생성\n",
    "#     delta_m_scaled = delta * 0.5\n",
    "    \n",
    "#     results = {}\n",
    "\n",
    "#     latent = freqmark.vae.encode(2*original-1).latent_dist.sample()\n",
    "#     latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "\n",
    "#     for name, delta in [(\"Optimized\", delta), \n",
    "#                         (\"Random\", delta_m_random), \n",
    "#                         (\"Scaled\", delta_m_scaled)]:\n",
    "        \n",
    "#         final_fft = latent_fft + delta\n",
    "#         final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "#         watermarked_image = (freqmark.vae.decode(final_latent).sample + 1) / 2\n",
    "        \n",
    "#         watermarked_image = F.interpolate(watermarked_image, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "#         logits = freqmark.decode_watermark(watermarked_image) \n",
    "        \n",
    "#         # 워터마크가 있어야 할 영역(gt_mask=1)의 평균 logit 점수 계산\n",
    "#         # avg_logit = (logits * gt_mask_resized).sum() / gt_mask_resized.sum()\n",
    "#         # results[name] = avg_logit.item()\n",
    "#         save_image(logits, f\"logits_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c890fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Average on 5 trials ##\n",
      "PSNR (imperceptibility): 29.60 dB\n",
      "IoU (localization accuracy): 0.9119\n"
     ]
    }
   ],
   "source": [
    "print(f\"## Average on {trials} trials ##\")\n",
    "print(f\"PSNR (imperceptibility): {np.mean(psnrs):.2f} dB\")\n",
    "print(f\"IoU (localization accuracy): {np.mean(ious):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bad2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "mask = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "save_image(original, \"eval_original.png\")\n",
    "save_image(watermarked, \"eval_watermarked.png\")\n",
    "save_image(img_edit, \"eval_edited_w_mask.png\")\n",
    "save_image(decoded_batch, \"eval_localized.png\")\n",
    "save_image(1-mask, \"eval_mask.png\")\n",
    "save_image(torch.abs(img_edit-original)*10, \"eval_edit-ori.png\")\n",
    "save_image(torch.abs(watermarked-original)*10, \"eval_wm-ori.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84742381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_logits = torch.cat(logits, dim=0)\n",
    "# # torch.save(total_logits, \"logits_w_l1_loss.pt\")\n",
    "# total_logits = total_logits.cpu().numpy().flatten()\n",
    "# print(f\"Mean: {total_logits.mean():.2f}, Std: {total_logits.std():.2f}, Min: {total_logits.min():.2f}, Max: {total_logits.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = torch.sigmoid(torch.cat(logits, dim=0)).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c50d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2571546/1549856749.py:7: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sig, bins=50, alpha=0.7)#, label='A: w/ L1 loss')\n",
    "plt.title('Logit Distribution Comparison')\n",
    "plt.xlabel('Logit Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('logits_comparison.png')\n",
    "# print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04f96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_b = torch.load(\"logits_w_l1_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_c = total_logits\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {logits_a.mean():.2f}, Std: {logits_a.std():.2f}, Min: {logits_a.min():.2f}, Max: {logits_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {logits_b.mean():.2f}, Std: {logits_b.std():.2f}, Min: {logits_b.min():.2f}, Max: {logits_b.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss (Dual)] Mean: {logits_c.mean():.2f}, Std: {logits_c.std():.2f}, Min: {logits_c.min():.2f}, Max: {logits_c.max():.2f}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(logits_a, bins=50, alpha=0.4, label='A: w/o L')\n",
    "# plt.hist(logits_b, bins=50, alpha=0.4, label='B: w/ L')\n",
    "# plt.hist(logits_c, bins=50, alpha=0.4, label='B: w/ L (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c61381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_a = torch.sigmoid(torch.load(\"logits_wo_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_b = torch.sigmoid(torch.load(\"logits_w_l1_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_c = torch.sigmoid(torch.load(\"logits_w_l1_loss_dual.pt\")).cpu().numpy().flatten()\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {sig_a.mean():.2f}, Std: {sig_a.std():.2f}, Min: {sig_a.min():.2f}, Max: {sig_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {sig_b.mean():.2f}, Std: {sig_b.std():.2f}, Min: {sig_b.min():.2f}, Max: {sig_b.max():.2f}\")\n",
    "# print(f\"[C: w/ L1 Loss (Dual)] Mean: {sig_c.mean():.2f}, Std: {sig_c.std():.2f}, Min: {sig_c.min():.2f}, Max: {sig_c.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d08685",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2196290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(sig_a, bins=50, alpha=0.7, label='A: w/o Add. Loss')\n",
    "# plt.hist(sig_b, bins=50, alpha=0.7, label='B: w/ L1 Loss')\n",
    "# plt.hist(sig_c, bins=50, alpha=0.7, label='B: w/ L1 Loss (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n",
    "# # print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02533add",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca09af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
